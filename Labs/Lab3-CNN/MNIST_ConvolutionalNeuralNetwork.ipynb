{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Convolutional Neural Network with MNIST\n",
    "\n",
    "\n",
    "# Model Overview\n",
    "\n",
    "In this lab we will train a Convolutional Neural Network (CNN) on MNIST data. \n",
    "\n",
    "The lab comprises two parts. During the first part, the instructor will walk you through the code to define, train, and evaluate the initial version of the CNN model. In the second part you will compete with other students to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29b9bd1d-766f-4422-ad96-de0accc1ce58"
    }
   },
   "source": [
    "\n",
    "## Overview of convolutional neural networks\n",
    "A [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN, or ConvNet) is a type of [feed-forward](https://en.wikipedia.org/wiki/Feedforward_neural_network) artificial neural network. The CNNs take advantage of the spatial nature of the data. In nature, we perceive different objects by their shapes, size and colors. For example, objects in a natural scene are typically edges, corners/vertices (defined by two of more edges), color patches etc. These primitives are often identified using different detectors (e.g., edge detection, color detector) or combination of detectors interacting to facilitate image interpretation (object classification, region of interest detection, scene description etc.) in real world vision related tasks. These detectors are also known as filters. Convolution is a mathematical operator that takes an image and a filter as input and produces a filtered output (representing say egdges, corners, colors etc in the input image).  Historically, these filters are a set of weights that were often hand crafted or modeled with mathematical functions (e.g., [Gaussian](https://en.wikipedia.org/wiki/Gaussian_filter) / [Laplacian](http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm) / [Canny](https://en.wikipedia.org/wiki/Canny_edge_detector) filter).  The filter outputs are mapped through non-linear activation functions mimicking human brain cells called [neurons](https://en.wikipedia.org/wiki/Neuron).\n",
    "\n",
    "Convolutional networks provide a machinery to learn these filters from the data directly instead of explicit mathematical models and have been found to be superior (in real world tasks) compared to historically crafted filters.  With convolutional networks, the focus is on learning the filter weights instead of learning individually fully connected pair-wise (between inputs and outputs) weights. In this way, the number of weights to learn is reduced when compared with the traditional MLP networks from the previous lab.  In a convolutional network, one learns several filters ranging from few single digits to few thousands depending on the network complexity.\n",
    "\n",
    "Many of the CNN primitives have been shown to have a conceptually parallel components in brain's [visual cortex](https://en.wikipedia.org/wiki/Visual_cortex). The group of neurons cells in visual cortex emit responses when stimulated. This region is known as the receptive field (RF). Equivalently, in convolution the input region corresponding to the filter dimensions can be considered as the receptive field. Popular deep CNNs or ConvNets (such as [AlexNet](https://en.wikipedia.org/wiki/AlexNet), [VGG](https://arxiv.org/abs/1409.1556), [Inception](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf), [ResNet](https://arxiv.org/pdf/1512.03385v1.pdf)) that are used for various [computer vision](https://en.wikipedia.org/wiki/Computer_vision) tasks have many of these architectural primitives (inspired from biology).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Model Creation'></a>\n",
    "## CNN Model Creation\n",
    "\n",
    "CNN is a feedforward network made up of bunch of layers in such a way that the output of one layer becomes the input to the next layer (similar to MLP). In MLP, all possible pairs of input pixels are connected to the output nodes with each pair having a weight, thus leading to a combinatorial explosion of parameters to be learnt and also increasing the possibility of overfitting ([details](http://cs231n.github.io/neural-networks-1/)). Convolution layers take advantage of the spatial arrangement of the pixels and learn multiple filters that significantly reduce the amount of parameters in the network ([details](http://cs231n.github.io/convolutional-networks/)). The size of the filter is a parameter of the convolution layer.  \n",
    "\n",
    "In this section, we introduce the basics of convolution operations. We show the illustrations in the context of RGB images (3 channels), eventhough the MNIST data we are using in this lab is a grayscale image (single channel).\n",
    "\n",
    "![input-rgb](https://www.cntk.ai/jup/cntk103d_rgb.png)\n",
    "\n",
    "### Convolution Layer\n",
    "\n",
    "A convolution layer is a set of filters. Each filter is defined by a weight (**W**) matrix, and  bias ($b$).\n",
    "\n",
    "![input-filter](https://www.cntk.ai/jup/cntk103d_filterset.png)\n",
    "\n",
    "These filters are scanned across the image performing the dot product between the weights and corresponding input value ($\\vec{x}^T$). The bias value is added to the output of the dot product and the resulting sum is optionally mapped through an activation function. This process is illustrated in the following animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(url=\"https://www.cntk.ai/jup/cntk103d_conv2d_final.gif\", width= 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution layers incorporate following key features:\n",
    "\n",
    "   - Instead of being fully-connected to all pairs of input and output nodes , each convolution node is **locally-connected** to a subset of input nodes localized to a smaller input region, also referred to as receptive field (RF). The figure above illustrates a small 3 x 3 regions in the image as the RF region. In the case of an RGB, image there would be three such 3 x 3 regions, one each of the 3 color channels. \n",
    "   \n",
    "   \n",
    "   - Instead of having a single set of weights (as in a Dense layer), convolutional layers have multiple sets (shown in figure with multiple colors), called **filters**. Each filter detects features within each possible RF in the input image.  The output of the convolution is a set of `n` sub-layers (shown in the animation above) where `n` is the number of filters (refer to the above figure).  \n",
    "   \n",
    "     \n",
    "   - Within a sublayer, instead of each node having its own set of weights, a single set of **shared weights** are used by all nodes in that sublayer. This reduces the number of parameters to be learnt and thus overfitting. This also opens the door for several aspects of deep learning which has enabled very practical solutions to be built:\n",
    "    -- Handling larger images (say 512 x 512)\n",
    "    - Trying larger filter sizes (corresponding to a larger RF) say 11 x 11\n",
    "    - Learning more filters (say 128)\n",
    "    - Explore deeper architectures (100+ layers)\n",
    "    - Achieve translation invariance (the ability to recognize a feature independent of where they appear in the image). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strides and Pad parameters\n",
    "\n",
    "**How are filters positioned?** In general, the filters are arranged in overlapping tiles, from left to right, and top to bottom.  Each convolution layer has a parameter to specify the `filter_shape`, specifying the width and height of the filter in case most natural scene images.  There is a parameter (`strides`) that controls the how far to step to right when moving the filters through multiple RF's in a row, and how far to step down when moving to the next row.  The boolean parameter `pad` controls if the input should be padded around the edges to allow a complete tiling of the RF's near the borders. \n",
    "\n",
    "The animation above shows the results with a `filter_shape` = (3, 3), `strides` = (2, 2) and `pad` = False. The two animations below show the results when `pad` is set to True. First, with a stride of 2 and second having a stride of 1.\n",
    "Note: the shape of the output (the teal layer) is different between the two stride settings. Many a times your decision to pad and the stride values to choose are based on the shape of the output layer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot images with strides of 2 and 1 with padding turned on\n",
    "images = [(\"https://www.cntk.ai/jup/cntk103d_padding_strides.gif\" , 'With stride = 2'),\n",
    "          (\"https://www.cntk.ai/jup/cntk103d_same_padding_no_strides.gif\", 'With stride = 1')]\n",
    "\n",
    "for im in images:\n",
    "    print(im[1])\n",
    "    display(Image(url=im[0], width=200, height=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "\n",
    "Often a times, one needs to control the number of parameters especially when having deep networks. For every layer of the convolution layer output (each layer, corresponds to the output of a filter), one can have a pooling layer. Pooling layers are typically introduced to:\n",
    "- Reduce the dimensionality of the previous layer (speeding up the network),\n",
    "- Makes the model more tolerant to changes in object location in the image. For example, even when a digit is shifted to one side of the image instead of being in the middle, the classifer would perform the classification task well.\n",
    "\n",
    "The calculation on a pooling node is much simpler than a normal feedforward node.  It has no weight, bias, or activation function.  It uses a simple aggregation function (like max or average) to compute its output.  The most commonly used function is \"max\" - a max pooling node simply outputs the maximum of the input values corresponding to the filter position of the input. The figure below shows the input values in a 4 x 4 region. The max pooling window size is 2 x 2 and starts from the top left corner. The maximum value within the window becomes the output of the region. Every time the model is shifted by the amount specified by the stride parameter (as shown in the figure below) and the maximum pooling operation is repeated. \n",
    "![maxppool](https://cntk.ai/jup/201/MaxPooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Walkthrough\n",
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "138d1a78-02e2-4bd6-a20e-07b83f303563"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import cntk as C\n",
    "\n",
    "# Ensure we always get the same amount of randomness\n",
    "np.random.seed(0)\n",
    "\n",
    "C.device.try_set_default_device(C.device.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "\n",
    "\n",
    "In the previous labs, as shown below, we have always flattened the input image into a vector.  With convoultional networks, we do not flatten the image in this way.\n",
    "\n",
    "![MNIST-flat](https://www.cntk.ai/jup/cntk103a_MNIST_input.png)\n",
    "\n",
    "**Input Dimensions**:  \n",
    "\n",
    "In convolutional networks for images, the input data is often shaped as a 3D matrix (number of channels, image width, height), which preserves the spatial relationship between the pixels. In the figure above, the MNIST image is a single channel (grayscale) data, so the input dimension is specified as a (1, image width, image height) tuple. \n",
    "\n",
    "![input-rgb](https://www.cntk.ai/jup/cntk103d_rgb.png)\n",
    "\n",
    "Natural scene color images are often presented as Red-Green-Blue (RGB) color channels. The input dimension of such images are specified as a (3, image width, image height) tuple. If one has RGB input data as a volumetric scan with volume width, volume height and volume depth representing the 3 axes, the input data format would be specified by a tuple of 4 values (3, volume width, volume height, volume depth). In this way CNTK enables specification of input images in arbitrary higher-dimensional space.\n",
    "\n",
    "Since our training data is stored on our local machine in the CNTK CTF format,\n",
    "    |labels 0 0 0 1 0 0 0 0 0 0 |features 0 255 0 123 ... \n",
    "                                                  (784 integers each representing a pixel gray level)\n",
    "    \n",
    "we will need to reshape our data into a 3D matrix when defining the input variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_file = \"../Data/MNIST_train.txt\"\n",
    "validation_file = \"../Data/MNIST_validate.txt\"\n",
    "test_file = '../Data/MNIST_test.txt'\n",
    "\n",
    "# Define a reader for the CTF formatted MNIST files \n",
    "def create_reader(path, is_training, input_dim, label_dim):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        features  = C.io.StreamDef(field='features', shape=input_dim),\n",
    "        labels    = C.io.StreamDef(field='labels',   shape=label_dim)\n",
    "    )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition and training\n",
    "### Define the network\n",
    "\n",
    "\n",
    "The model we are going to build in the lab is a simple convolution network containing a set of alternating convolution and pooling layers followed by a dense output layer for classification. You will find variants of this structure in many classical deep networks (VGG, AlexNet, etc.)\n",
    "\n",
    "The illustrations are presented in the context of 2-dimensional (2D) images, but the concept and the CNTK components can operate on any dimensional data. The above schematic shows 2 convolution layer and 2 max-pooling layers. A typical strategy is to increase the number of filters in the deeper layers while reducing the spatial size of each intermediate layers.\n",
    "\n",
    "![mnist-conv-mp](http://www.cntk.ai/jup/conv103d_mnist-conv-mp.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_cnn_network(input_shape, num_output_classes):\n",
    "    # Create inputs\n",
    "    features = C.input_variable(input_shape)\n",
    "    \n",
    "    # Scale the input features\n",
    "    feature_scale = 1.0/256.0\n",
    "    features_norm = C.element_times(feature_scale, features) \n",
    "    \n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.ops.relu):\n",
    "        network_template = C.layers.Sequential([\n",
    "            C.layers.Convolution2D(filter_shape=(5,5), num_filters=8, strides=(1,1), pad=True, name='first_conv'),\n",
    "            C.layers.MaxPooling(filter_shape=(2,2), strides=(2,2), name='first_max'),\n",
    "            C.layers.Convolution2D(filter_shape=(5,5), num_filters=16, strides=(1,1), pad=True, name='sec_conv'),\n",
    "            C.layers.MaxPooling(filter_shape=(2,2), strides=(2,2), name='second_max'),\n",
    "            C.layers.Dense(num_output_classes, activation=None, name='classify')\n",
    "        ])\n",
    "    z = network_template(features_norm)\n",
    "    return z\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and visualize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_shape = (1, 28, 28)\n",
    "num_output_classes = 10\n",
    "\n",
    "z = create_cnn_network(input_shape, num_output_classes)\n",
    "C.logging.graph.plot(z, \"graph.png\")\n",
    "Image(\"graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding number of model parameters to be estimated is key to deep learning since there is a direct dependency on the amount of data one needs to have. You need more data for a model that has larger number of parameters to prevent overfitting. In other words, with a fixed amount of data, one has to constrain the number of parameters. There is no golden rule between the amount of data one needs for a model. However, there are ways one can boost performance of model training with [data augmentation](https://deeplearningmania.quora.com/The-Power-of-Data-Augmentation-2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of parameters in the network\n",
    "C.logging.log_number_of_parameters(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has 2 convolution layers each having a weight and bias. This adds up to 4 parameter tensors. Additionally the dense layer has weight and bias tensors. Thus, the 6 parameter tensors.\n",
    "\n",
    "Let us now count the number of parameters:\n",
    "- *First convolution layer*: There are 8 filters each of size (1 x 5 x 5) where 1 is the number of channels in the input image. This adds up to 200 values in the weight matrix and 8 bias values.\n",
    "\n",
    "\n",
    "- *Second convolution layer*: There are 16 filters each of size (8 x 5 x 5) where 8 is the number of channels in the input to the second layer (= output of the first layer). This adds up to 3200 values in the weight matrix and 16 bias values.\n",
    "\n",
    "\n",
    "- *Last dense layer*: There are 16 x 7 x 7 input values and it produces 10 output values corresponding to the 10 digits in the MNIST dataset. This corresponds to (16 x 7 x 7) x 10 weight values and 10 bias values.\n",
    "\n",
    "Adding these up gives the 11274 parameters in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure training\n",
    "\n",
    "In this lab we have modified the **trainer** and the **training session** code to demonstrate how to progressively change learning rate and minibatch size.\n",
    "\n",
    "The learning rate is specified as a list (`[0.001]*12 + [0.0005]*6 +`...). Together with the `epoch_size` parameter, this tells CNTK to use 0.001 for 12 epochs, and then continue with 0.0005 for another 6, etc.\n",
    "\n",
    "Similarly, the minibatch size is progressively increased as we progress with training.\n",
    "\n",
    "#### Configure a trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_trainer(network, labels, epoch_size):\n",
    "    ## Define loss and metric\n",
    "    loss = C.cross_entropy_with_softmax(network, labels)\n",
    "    metric = C.classification_error(network, labels)\n",
    "\n",
    "    # Create an SGD learner\n",
    "    lr_schedule = C.learning_rate_schedule([0.002]*12 + [0.001]*6  + [0.0005]*3 + [0.000025],\n",
    "                                           C.UnitType.sample,\n",
    "                                           epoch_size=epoch_size\n",
    "                                          )\n",
    "    \n",
    "    learner = C.sgd(network.parameters, lr_schedule)\n",
    "\n",
    "    # Create a progress printing helper\n",
    "    progress_printer = C.logging.ProgressPrinter()\n",
    "\n",
    "    # Create a trainer\n",
    "    return C.Trainer(network, (loss, metric), [learner], [progress_printer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure a training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_validate(network, training_file, validation_file, mb_schedule, epoch_size, num_epochs):\n",
    "\n",
    "    # Extract input and output dimensions\n",
    "    input_shape = network.arguments[0].shape\n",
    "    input_dim = input_shape[1] * input_shape[2]\n",
    "    num_output_classes = network.outputs[0].shape[0]\n",
    "    \n",
    "    # Create the training and validation data set readers\n",
    "    reader_train = create_reader(training_file, True, input_dim, num_output_classes)\n",
    "    reader_validate = create_reader(validation_file, False, input_dim, num_output_classes)\n",
    "\n",
    "     # Define mappings from reader streams to network and ground truth inputs\n",
    "    features = network.arguments[0]\n",
    "    labels = C.input_variable(num_output_classes, is_sparse=True)\n",
    "   \n",
    "    input_map_training = {\n",
    "        features: reader_train.streams.features,\n",
    "        labels: reader_train.streams.labels\n",
    "    }\n",
    "\n",
    "    # Define mappings from reader streams to network inputs\n",
    "    input_map_validation = {\n",
    "        features: reader_validate.streams.features,\n",
    "        labels: reader_validate.streams.labels\n",
    "    }\n",
    "    \n",
    "    # Create a trainer\n",
    "    trainer = create_trainer(network, labels, epoch_size)\n",
    "\n",
    "    # Set up cross-validation configuration\n",
    "    cv_config = C.CrossValidationConfig(minibatch_source=reader_validate,\n",
    "                                        model_inputs_to_streams = input_map_validation,\n",
    "                                        frequency=None)\n",
    "\n",
    "    # Create a training session\n",
    "    training_sess = C.training_session(trainer=trainer,\n",
    "                                 mb_source=reader_train,\n",
    "                                 mb_size=mb_schedule,\n",
    "                                 model_inputs_to_streams=input_map_training,\n",
    "                                 progress_frequency=epoch_size,\n",
    "                                 max_samples=epoch_size * num_epochs,\n",
    "                                 cv_config=cv_config \n",
    "                                )\n",
    "\n",
    "    training_sess.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "epoch_size = 6400\n",
    "num_epochs = 50\n",
    "mb_schedule = C.minibatch_size_schedule([64]*12 + [128]*6 + [256], epoch_size=epoch_size)\n",
    "\n",
    "train_and_validate(z, training_file, validation_file, mb_schedule, epoch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hackathon\n",
    "\n",
    "Try to improve the performance of the model. \n",
    "\n",
    "Hints:\n",
    "- Try different number of feature maps\n",
    "- Try different number of convolutional layers\n",
    "- Try two dense layers in the final classification part of the network\n",
    "- Try using Dropout layer to control overfitting\n",
    "\n",
    "\n",
    "## Final testing\n",
    "\n",
    "\n",
    "DON'T CHEAT. DON'T USE MNIST_test.txt FOR MODEL TRAINING AND SELECTION. DON'T EXECUTE THE BELOW CELL TILL YOU ARE READY FOR THE FINAL TEST\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def final_evaluation(network, test_file):\n",
    "    \n",
    "    \n",
    "    # Extract input and output dimensions\n",
    "    input_shape = network.arguments[0].shape\n",
    "    input_dim = input_shape[1] * input_shape[2]\n",
    "    num_output_classes = network.outputs[0].shape[0]\n",
    "    \n",
    "    # Create the test data set readers\n",
    "    reader = create_reader(test_file, False,  input_dim, num_output_classes)\n",
    "\n",
    "     # Define mappings from reader streams to network and ground truth inputs\n",
    "    features = network.arguments[0]\n",
    "    labels = C.input_variable(num_output_classes, is_sparse=True)\n",
    "   \n",
    "    input_map = {\n",
    "        features: reader.streams.features,\n",
    "        labels: reader.streams.labels\n",
    "    }\n",
    "   \n",
    "    metric = C.classification_error(network, labels)\n",
    "    \n",
    "    evaluator = C.Evaluator(metric, [C.logging.ProgressPrinter()])\n",
    "    \n",
    "    minibatch_size = 1024\n",
    "    data = reader.next_minibatch(minibatch_size, input_map=input_map)\n",
    "    while bool(data):\n",
    "        evaluator.test_minibatch(data)\n",
    "        data = reader.next_minibatch(minibatch_size, input_map=input_map)\n",
    "    evaluator.summarize_test_progress()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "final_evaluation(z, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
