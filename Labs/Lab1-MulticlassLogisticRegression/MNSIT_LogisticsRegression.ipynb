{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29b9bd1d-766f-4422-ad96-de0accc1ce58"
    }
   },
   "source": [
    "# Lab 1 - Logistic Regression with MNIST\n",
    "\n",
    "\n",
    "# Lab Overview\n",
    "In this lab we will build and train the Multiclass Logistic Regression model using MNIST data. \n",
    "\n",
    "The lab comprises two parts. During the first part, the instructor will walk you through the code to define, train, and evaluate the initial version of MLR model. In the second part you will compete with other students to improve the performance of the model.\n",
    "\n",
    "The MNIST data consists of hand-written digits with little background noise making it a standard dataset to create, experiment and learn deep learning models with reasonably small comptuing resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\", width=200, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) (LR) is a fundamental machine learning technique that uses a linear weighted combination of features and generates probability-based predictions of different classes.  \n",
    "â€‹\n",
    "There are two basic forms of LR: **Binary LR** (with a single output that can predict two classes) and **multinomial LR** (with multiple outputs, each of which is used to predict a single class).  \n",
    "\n",
    "![LR-forms](http://www.cntk.ai/jup/cntk103b_TwoFormsOfLR-v3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Binary Logistic Regression** (see top of figure above), the input features are each scaled by an associated weight and summed together.  The sum is passed through a squashing (aka activation) function and generates an output in [0,1].  This output value (which can be thought of as a probability) is then compared with a threshold (such as 0.5) to produce a binary label (0 or 1).  This technique supports only classification problems with two output classes, hence the name binary LR.  In the binary LR example shown above, the [sigmoid][] function is used as the squashing function.\n",
    "\n",
    "[sigmoid]: https://en.wikipedia.org/wiki/Sigmoid_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Multiclass Linear Regression** (see bottom of figure above), 2 or more output nodes are used, one for each output class to be predicted.  Each summation node uses its own set of weights to scale the input features and sum them together. Instead of passing the summed output of the weighted input features through a sigmoid squashing function, the output is often passed through a `softmax` function (which in addition to squashing, like the `sigmoid`, the `softmax` normalizes each nodes' output value using the sum of all unnormalized nodes). \n",
    "\n",
    "$$ h(\\textbf{z})_i = \\frac{e^{z_i}}{\\sum_{k=1}^C{e^{z_k}}} \\text{,  where  } \\textbf{z} = \\textbf{W} \\times \\textbf{x}  + \\textbf{b} $$\n",
    "\n",
    "In this lab, we will use multinomial LR for classifying the MNIST digits (0-9) using 10 output nodes (1 for each of our output classes).\n",
    "\n",
    "The figure below summarizes the model in the context of the MNIST data.\n",
    "\n",
    "![mnist-LR](https://www.cntk.ai/jup/cntk103b_MNIST_LR.png)\n",
    "\n",
    "During the model training the values of **W** and **b** parameters are optimized to fit the training samples and generalize well to the samples outside of the training set.\n",
    "\n",
    "This is achieved by iteratively reducing the `loss` function - a function that expresses the \"difference\" between model predictions and training ground-truth labels. \n",
    "\n",
    "`Cross-entropy` is a popular function to measure the loss. It is defined as:\n",
    "\n",
    "$$ H(\\textbf{W},\\textbf{b}) = - \\sum_{j=1}^C y_j \\log (h(\\textbf{z})_j ) \\text{, where }  y_j \\text{ is a ground truth label} $$  \n",
    "\n",
    "Various optimization approaches can be utilized, Stochastic Gradient Descent (`sgd`) being one of the most popular ones. \n",
    "\n",
    "The \"classical\" stochastic gradient descent uses a single observation to calculate gradient estimates and update the model parameters. This approach is  attractive since it does not require the entire data set (all observation) to be loaded in memory and also requires gradient computation over fewer datapoints, thus allowing for training on large data sets. However, the updates generated using a single observation sample at a time can vary wildly between iterations. An intermediate ground is to use a small set of observations and use an average of the `loss` or error from that set to update the model parameters. This subset is called a *minibatch*.\n",
    "\n",
    "With minibatches, we often sample observation from the larger training dataset. We repeat the process of model parameters update using different combination of training samples and over a period of time minimize the `loss` (and the error). When the incremental error rates are no longer changing significantly or after a preset number of maximum minibatches to train, we claim that our model is trained.\n",
    "\n",
    "In the lab we will use the minibatch form of SGD. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "138d1a78-02e2-4bd6-a20e-07b83f303563"
    }
   },
   "outputs": [],
   "source": [
    "# Import the relevant components\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "\n",
    "# Ensure we always get the same amount of randomness\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "\n",
    "In this lab we are using the MNIST data pre-processed to follow CNTK CTF format. \n",
    "\n",
    "\n",
    "    |labels 0 0 0 0 0 0 0 1 0 0 |features 0 0 0 0 ... \n",
    "                                                  (784 integers each representing a pixel)\n",
    "                                                 \n",
    "\n",
    "Each line in the file contains two key-value pairs, also refered as streams. The `labels` stream is the one-hot encoded representation of a digit 0-9. The `features` stream is a 784 vector of 0-255 integers representing 28 x 28 pixel grayscale image.\n",
    "\n",
    "Our dataset includes three files: the training file with 50,000 images, the validation file with 10,000 images, and the testing file with 10,000 images.\n",
    "\n",
    "To read/sample the files, we define a `create_reader` function that configures and returns the CNTK MinibatchSource object.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_file = \"../Data/MNIST_train.txt\"\n",
    "validation_file = \"../Data/MNIST_validate.txt\"\n",
    "test_file = '../Data/MNIST_test.txt'\n",
    "\n",
    "\n",
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    labelStream = C.io.StreamDef(field='labels', shape=num_label_classes)\n",
    "    featureStream = C.io.StreamDef(field='features', shape=input_dim)\n",
    "    deserializer = C.io.CTFDeserializer(path, C.io.StreamDefs(labels = labelStream, features = featureStream))\n",
    "    return C.io.MinibatchSource(deserializer,\n",
    "       randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition and training\n",
    "In CNTK, a computational network (e.g. a neural network) is a **Function** object. On one hand a computational network in CNTK is just a function that you can call to apply to data. On the other hand, a computational network contains learnable parameters that can be accessed like object members. Complicated networks can be composed as hierarchies of simpler ones, which, for example, represent layers. \n",
    "\n",
    "CNTK function objects are represented internally as graph structures in C++ that encode the computation. This graph structure is wraped in the Python class `Function` that exposes the necessary interface so that other Python functions can call it and access its members (such as learnable parameters)\n",
    "\n",
    "The **Function** object is CNTK's single abstraction used to represent different operations from simple **basic operations** without learnable parameters to  **layers**, **recurrent step functions**, **complete models**, **criterion functions** and more.\n",
    "\n",
    "\n",
    "CNTK allows you to create networks using two styles of APIs: Functional API and Graph API.\n",
    "\n",
    "In this lab we will demonstrate the lower level Graph API. This API is more verbose but sometimes more flexible. In the following labs we will mostly use the Functional API style which is more concise and makes it easier to define complex network topologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_mlr_network(input_dim, num_output_classes):\n",
    "    # Create inputs \n",
    "    features = C.input_variable(input_dim)\n",
    "\n",
    "    # Scale the input features\n",
    "    feature_scale = 1.0/256.0\n",
    "    features_norm = C.element_times(feature_scale, features)\n",
    "\n",
    "    # Define parameters\n",
    "    W = C.parameter(shape=(input_dim, num_output_classes))\n",
    "    b = C.parameter(shape=(num_output_classes))\n",
    "\n",
    "    # And the network\n",
    "    z = C.times(features_norm, W) + b\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and visualize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "num_output_classes = 10\n",
    "\n",
    "z = create_mlr_network(input_dim, num_output_classes)\n",
    "C.logging.graph.plot(z, \"graph.png\")\n",
    "Image(\"graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure training\n",
    "#### Configure a trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_trainer(network, labels):\n",
    "    ## Define loss and metric\n",
    "    loss = C.cross_entropy_with_softmax(network, labels)\n",
    "    metric = C.classification_error(network, labels)\n",
    "\n",
    "    # Create an SGD learner\n",
    "    lr_schedule = C.learning_rate_schedule(0.2, C.UnitType.minibatch)\n",
    "    learner = C.sgd(network.parameters, lr_schedule)\n",
    "\n",
    "    # Create a progress printing helper\n",
    "    progress_printer = C.logging.ProgressPrinter()\n",
    "\n",
    "    # Create a trainer\n",
    "    return C.Trainer(network, (loss, metric), [learner], [progress_printer])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure a training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_and_validate(network, training_file, validation_file, mb_schedule, epoch_size, num_epochs):\n",
    "\n",
    "    # Extract input and output dimensions\n",
    "    input_dim = network.arguments[0].shape[0]\n",
    "    num_output_classes = network.outputs[0].shape[0]\n",
    "    \n",
    "    # Create the training and validation data set readers\n",
    "    reader_train = create_reader(training_file, True, input_dim, num_output_classes)\n",
    "    reader_validate = create_reader(validation_file, False, input_dim, num_output_classes)\n",
    "\n",
    "     # Define mappings from reader streams to network and ground truth inputs\n",
    "    features = network.arguments[0]\n",
    "    labels = C.input_variable(num_output_classes, is_sparse=True)\n",
    "   \n",
    "    input_map_training = {\n",
    "        features: reader_train.streams.features,\n",
    "        labels: reader_train.streams.labels\n",
    "    }\n",
    "\n",
    "    # Define mappings from reader streams to network inputs\n",
    "    input_map_validation = {\n",
    "        features: reader_validate.streams.features,\n",
    "        labels: reader_validate.streams.labels\n",
    "    }\n",
    "    \n",
    "    # Create a trainer\n",
    "    trainer = create_trainer(network, labels)\n",
    "\n",
    "    # Set up cross-validation configuration\n",
    "    cv_config = C.CrossValidationConfig(minibatch_source=reader_validate,\n",
    "                                        model_inputs_to_streams = input_map_validation,\n",
    "                                        frequency=None)\n",
    "\n",
    "    # Create a training session\n",
    "    training_sess = C.training_session(trainer=trainer,\n",
    "                                 mb_source=reader_train,\n",
    "                                 mb_size=mb_schedule,\n",
    "                                 model_inputs_to_streams=input_map_training,\n",
    "                                 progress_frequency=epoch_size,\n",
    "                                 max_samples=epoch_size * num_epochs,\n",
    "                                 cv_config=cv_config \n",
    "                                )\n",
    "\n",
    "    training_sess.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "epoch_size = 6400\n",
    "num_epochs = 100\n",
    "mb_schedule = 64\n",
    "\n",
    "train_and_validate(z, training_file, validation_file, mb_schedule, epoch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "    The trained network can be used like a \"normal Python function\" to score new data. Since CNTK supports `numpy` interoperability you can pass `numpy` arrays as input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read some data for scoring\n",
    "\n",
    "reader = create_reader(test_file, False,  input_dim, num_output_classes)\n",
    "\n",
    "input_map = {\n",
    "        input: reader.streams.features\n",
    "    }\n",
    "\n",
    "data = reader.next_minibatch(10, input_map = input_map)\n",
    "\n",
    "img_data = data[input].asarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add softmax node and call the network on an image tensor\n",
    "out = C.softmax(z)\n",
    "pred = out(img_data[0])\n",
    "\n",
    "print(pred)\n",
    "print(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon\n",
    "\n",
    "Try to improve the performance of the model. \n",
    "\n",
    "Hints:\n",
    "- Play with the learning rate, minibatch size and the number of epochs\n",
    "- You can look at regularization - check `l1_regularization` and `l2_regularization` hyper parameters of the `sgd` learner\n",
    "\n",
    "## Final testing\n",
    "\n",
    "\n",
    "DON'T CHEAT. DON'T USE MNIST_test.txt FOR MODEL TRAINING AND SELECTION. DON'T EXECUTE THE BELOW CELL TILL YOU ARE READY FOR THE FINAL TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def final_evaluation(network, test_file):\n",
    "    \n",
    "    # Extract input and output dimensions\n",
    "    input_dim = network.arguments[0].shape[0]\n",
    "    num_output_classes = network.outputs[0].shape[0]\n",
    "    \n",
    "    # Create the test data set readers\n",
    "    reader = create_reader(test_file, False,  input_dim, num_output_classes)\n",
    "\n",
    "     # Define mappings from reader streams to network and ground truth inputs\n",
    "    features = network.arguments[0]\n",
    "    labels = C.input_variable(num_output_classes, is_sparse=True)\n",
    "   \n",
    "    input_map = {\n",
    "        features: reader.streams.features,\n",
    "        labels: reader.streams.labels\n",
    "    }\n",
    "   \n",
    "    metric = C.classification_error(network, labels)\n",
    "    \n",
    "    evaluator = C.Evaluator(metric, [C.logging.ProgressPrinter()])\n",
    "    \n",
    "    minibatch_size = 1024\n",
    "    data = reader.next_minibatch(minibatch_size, input_map=input_map)\n",
    "    while bool(data):\n",
    "        evaluator.test_minibatch(data)\n",
    "        data = reader.next_minibatch(minibatch_size, input_map=input_map)\n",
    "    evaluator.summarize_test_progress()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_evaluation(z, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
